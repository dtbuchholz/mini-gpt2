# Model config
BLOCK_SIZE=1024  # max sequence length
VOCAB_SIZE=50304  # GPT-2 vocab size
N_LAYER=12  # number of layers
N_HEAD=12  # number of heads
N_EMBD=768  # embedding dimension

# Training hyperparameters from environment variables
TOTAL_BATCH_SIZE=524288  # 2**19, ~0.5M tokens
BATCH_SIZE=8  # micro batch size (B)
SEQ_LENGTH=256  # sequence length (T) 
MAX_LR=6e-4 # maximum learning rate
MIN_LR=6e-5 # minimum learning rate (explicitly set to 10% of MAX_LR) 
WARMUP_STEPS=715 # warmup steps for learning rate
MAX_STEPS=19073  # ~1 epoch for 10B tokens
SAVE_CHECKPOINT_INTERVAL=5000 # save checkpoint every 5000 steps
RUN_EVAL_INTERVAL=250 # run evaluation every 250 steps
GRAD_CLIP=1.0 # gradient clipping threshold for adaptive clipping
WEIGHT_DECAY=0.1 # weight decay for AdamW optimizer

# Dataset path and HuggingFace dataset name
DATASET_PATH="natural_reasoning"
HF_DATASET_NAME="facebook/natural_reasoning"
SHARD_SIZE=1e8  # 100M tokens per shard

# Model path for testing
TESTING_MODEL_PATH="log/" # default to the latest checkpoint in the log directory

# For example, if you want to train a toy model in a few minutes, you can use the following:
# BLOCK_SIZE=256
# N_LAYER=2
# N_HEAD=2
# N_EMBD=128

# TOTAL_BATCH_SIZE=65536
# BATCH_SIZE=8
# SEQ_LENGTH=256
# MAX_LR=6e-4
# WARMUP_STEPS=10
# MAX_STEPS=50
# WEIGHT_DECAY=0.1
# SAVE_CHECKPOINT_INTERVAL=10

# DATASET_PATH="natural_reasoning"
# HF_DATASET_NAME="facebook/natural_reasoning"
# SHARD_SIZE=1e8

# TESTING_MODEL_PATH="log/"