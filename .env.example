# Model config
BLOCK_SIZE=1024  # max sequence length
VOCAB_SIZE=50304  # GPT-2 vocab size
N_LAYER=12  # number of layers
N_HEAD=12  # number of heads
N_EMBD=768  # embedding dimension

# Training hyperparameters from environment variables
TOTAL_BATCH_SIZE=524288  # 2**19, ~0.5M tokens
BATCH_SIZE=8  # micro batch size (B)
SEQ_LENGTH=256  # sequence length (T)
MAX_LR=6e-4
MIN_LR=6e-5 # explicitly set to 10% of MAX_LR
WARMUP_STEPS=715
MAX_STEPS=19073  # ~1 epoch for 10B tokens
WEIGHT_DECAY=0.1

# Dataset path and HuggingFace dataset name
DATASET_PATH="natural_reasoning"
HF_DATASET_NAME="facebook/natural_reasoning"
SHARD_SIZE=1e8  # 100M tokens per shard

# Model path for testing
TESTING_MODEL_PATH="log/" # default to the latest checkpoint in the log directory

# For example, if you want to train a toy model in a few minutes, you can use the following:
# BLOCK_SIZE=256
# VOCAB_SIZE=50257
# N_LAYER=2
# N_HEAD=2
# N_EMBD=128

# TOTAL_BATCH_SIZE=65536
# BATCH_SIZE=8
# SEQ_LENGTH=256
# MAX_LR=6e-4
# MIN_LR=6e-5
# WARMUP_STEPS=10
# MAX_STEPS=50
# WEIGHT_DECAY=0.1

# DATASET_PATH="natural_reasoning"
# HF_DATASET_NAME="facebook/natural_reasoning"
# SHARD_SIZE=1e8

# TESTING_MODEL_PATH="log/"